{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WmmM69NycMX",
        "colab_type": "text"
      },
      "source": [
        "Q1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2havkizyeHY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5c1d9f67-f594-4de8-852f-b8c46301f9ca"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def roll_dice(prob=None):\n",
        "    if not prob:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6])\n",
        "    else:\n",
        "        return np.random.choice(a=[1,2,3,4,5,6],p=prob)\n",
        "           \n",
        "def simulate(n=100,prob=None):\n",
        "    reached=0\n",
        "    for _ in range(n):\n",
        "        throws=250\n",
        "        curr_pos=0\n",
        "        while throws:\n",
        "            throws-=1\n",
        "            curr_step=roll_dice(prob)\n",
        "            if curr_pos>60:\n",
        "                reached+=1\n",
        "                break\n",
        "                \n",
        "            if curr_step in {1,2}:\n",
        "                curr_pos-=1\n",
        "            elif curr_step in {3,4,5}:\n",
        "                curr_pos+=1\n",
        "            else:\n",
        "                curr_step=roll_dice(prob)\n",
        "                curr_pos+=curr_step\n",
        "    print(reached/n)\n",
        "simulate() #100 iterations in without probability weights \n",
        "simulate(prob=[0.2,0.3,0.2,0.1,0.1,0.1]) #100 iterations with probability weights\n",
        "\n",
        "simulate(1000)\n",
        "simulate(1000,[0.2,0.3,0.2,0.1,0.1,0.1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.28\n",
            "1.0\n",
            "0.306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tina91gymGD",
        "colab_type": "text"
      },
      "source": [
        "Q2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NivJpYEynT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db77b887-1db1-428d-9026-f35f9395fd4a"
      },
      "source": [
        "#randomly generated data for the Multiple Linear regression \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for p in range(n_features):\n",
        "  X_p = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_p)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.5 * X[0]) + eps + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Random data for the Logistic regression\n",
        "n_features_of_the_model = 4\n",
        "X = []\n",
        "for i in range(n_features_of_the_model):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "\n",
        "\n",
        "\n",
        "#Random data for K-mean clustering\n",
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.077613  2.161452  0.085531  0.849455  2.320998\n",
            "1 -0.025245  0.489524 -0.230003  0.061454  0.955830\n",
            "2  0.136042  0.437742  1.104734 -0.979875  1.306937\n",
            "3 -1.868668  2.478251  2.052069  0.383353  1.741575\n",
            "4  0.973540  0.084645 -0.765247  0.527924  1.459945\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.261397  0.441878 -2.353537 -0.402768  0.565594\n",
            "96  1.101744 -0.608329  0.257553  0.140512  1.146310\n",
            "97 -0.279349 -0.758384  0.483315 -0.668155  0.480659\n",
            "98 -0.402324 -0.300901 -0.635043  1.113297  0.726214\n",
            "99 -0.580962  0.733723  1.477383 -1.075774  1.138571\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.049733    0.081002    0.041212    0.034745    1.083402\n",
            "std      0.925462    0.993884    0.957429    1.051187    0.802079\n",
            "min     -2.289416   -2.850394   -2.353537   -2.239454   -0.529355\n",
            "25%     -0.633721   -0.536192   -0.708647   -0.879062    0.487924\n",
            "50%      0.068613   -0.012039    0.033823    0.060565    0.985696\n",
            "75%      0.597434    0.657412    0.705507    0.848337    1.569136\n",
            "max      2.409933    2.490955    2.052069    2.790015    3.351205\n",
            "         X0        X1        X2        X3         Y\n",
            "0  0.077613  2.161452  0.085531  0.849455  2.320998\n",
            "1 -0.025245  0.489524 -0.230003  0.061454  0.955830\n",
            "2  0.136042  0.437742  1.104734 -0.979875  1.306937\n",
            "3 -1.868668  2.478251  2.052069  0.383353  1.741575\n",
            "4  0.973540  0.084645 -0.765247  0.527924  1.459945\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.261397  0.441878 -2.353537 -0.402768  0.565594\n",
            "96  1.101744 -0.608329  0.257553  0.140512  1.146310\n",
            "97 -0.279349 -0.758384  0.483315 -0.668155  0.480659\n",
            "98 -0.402324 -0.300901 -0.635043  1.113297  0.726214\n",
            "99 -0.580962  0.733723  1.477383 -1.075774  1.138571\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.049733    0.081002    0.041212    0.034745    1.083402\n",
            "std      0.925462    0.993884    0.957429    1.051187    0.802079\n",
            "min     -2.289416   -2.850394   -2.353537   -2.239454   -0.529355\n",
            "25%     -0.633721   -0.536192   -0.708647   -0.879062    0.487924\n",
            "50%      0.068613   -0.012039    0.033823    0.060565    0.985696\n",
            "75%      0.597434    0.657412    0.705507    0.848337    1.569136\n",
            "max      2.409933    2.490955    2.052069    2.790015    3.351205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.077613  2.161452  0.085531  0.849455  2.320998\n",
            "1 -0.025245  0.489524 -0.230003  0.061454  0.955830\n",
            "2  0.136042  0.437742  1.104734 -0.979875  1.306937\n",
            "3 -1.868668  2.478251  2.052069  0.383353  1.741575\n",
            "4  0.973540  0.084645 -0.765247  0.527924  1.459945\n",
            "          X0        X1        X2        X3         Y\n",
            "95 -0.261397  0.441878 -2.353537 -0.402768  0.565594\n",
            "96  1.101744 -0.608329  0.257553  0.140512  1.146310\n",
            "97 -0.279349 -0.758384  0.483315 -0.668155  0.480659\n",
            "98 -0.402324 -0.300901 -0.635043  1.113297  0.726214\n",
            "99 -0.580962  0.733723  1.477383 -1.075774  1.138571\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.049733    0.081002    0.041212    0.034745    1.083402\n",
            "std      0.925462    0.993884    0.957429    1.051187    0.802079\n",
            "min     -2.289416   -2.850394   -2.353537   -2.239454   -0.529355\n",
            "25%     -0.633721   -0.536192   -0.708647   -0.879062    0.487924\n",
            "50%      0.068613   -0.012039    0.033823    0.060565    0.985696\n",
            "75%      0.597434    0.657412    0.705507    0.848337    1.569136\n",
            "max      2.409933    2.490955    2.052069    2.790015    3.351205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BATYktYYzZzy",
        "colab_type": "text"
      },
      "source": [
        "Q3\n",
        "a)Linear regression using gradient descent\n",
        "b)Logistic regression using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKzKAIF3zp_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "ca9e1b65-aa9f-49e4-b3d0-9dd97579fa01"
      },
      "source": [
        "#linear regression using the gradient descent\n",
        "print(\" linear regrission using gradient descent are\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#logistic regression using gradient descent\n",
        "print(\"from logistic using gradient descent are\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " linear regrission using gradient descent are\n",
            "0.05488428708268233 0.1963032926941412\n",
            "\n",
            "\n",
            "\n",
            "from logistic using gradient descent are\n",
            "0.6931471805599453\n",
            "0.4395612799517858\n",
            "0.439455246742264\n",
            "0.4393517188397068\n",
            "0.43925063685755994\n",
            "0.43915194277403136\n",
            "0.439055579902928\n",
            "0.43896149286507696\n",
            "0.43886962756031334\n",
            "0.43877993114002384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lWGEn5OzuFs",
        "colab_type": "text"
      },
      "source": [
        "c)Linear Regression using L1 and L2 regularization                              \n",
        "d)Logistic regression using L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzeBMkDLzwJP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "3a10224b-81e0-4b44-c824-61df50295d20"
      },
      "source": [
        "print(\"Linear regression using L1 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Linear regression using L2 regularisation\")\n",
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "print(b1,b0)\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L1 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n",
        "\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print(\"Logistic regression using L2 regularisation\")\n",
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear regression using L1 regularisation\n",
            "0.04568112562556371 0.19634695966731272\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Linear regression using L2 regularisation\n",
            "0.05461930041870621 0.19630413970491087\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L1 regularisation\n",
            "0.6931471805599453\n",
            "0.04179837764995287\n",
            "-0.35072341533786455\n",
            "-0.7382626073953924\n",
            "-1.1208933010719615\n",
            "-1.4986895509248568\n",
            "-1.8717252582961392\n",
            "-2.2400740753677426\n",
            "-2.6038093178800814\n",
            "-2.9630038859236203\n",
            "\n",
            "\n",
            "\n",
            "Logistic regression using L2 regularisation\n",
            "0.6931471805599453\n",
            "0.43966935146785635\n",
            "0.43987819522229105\n",
            "0.4402828684436094\n",
            "0.44087050713600545\n",
            "0.44162889656951687\n",
            "0.44254644156177836\n",
            "0.44361213804298877\n",
            "0.44481554585127603\n",
            "0.44614676270775416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id6Vv9gy0P2S",
        "colab_type": "text"
      },
      "source": [
        "e)K-means Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqC1Qi2c0WIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "6868c1ea-3106-47b1-cad7-664ff6e7c251"
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.855682668482043\n",
            "146.64383217351633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk__3atd0b1b",
        "colab_type": "text"
      },
      "source": [
        "Q4\n",
        "a)Linear Regression from scratch using OOPS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X26D67Mi0gJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzCn7HQ10r6L",
        "colab_type": "text"
      },
      "source": [
        "b)Logistic Regression using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH7TtHFv0vbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iGmmJf40yy7",
        "colab_type": "text"
      },
      "source": [
        "c)K means from scratch using oops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo9vsiyp05X0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}